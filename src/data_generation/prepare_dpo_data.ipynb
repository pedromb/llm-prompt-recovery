{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_out = pkl.load(open(\"/home/pbernardo/github/llm-prompt-recovery/data/mistral_zero_shot_output.pkl\", \"rb\"))\n",
    "data = pd.read_csv(\"/home/pbernardo/github/llm-prompt-recovery/data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_split = json.load(open(\"/home/pbernardo/github/llm-prompt-recovery/data/cluster_to_split.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
    "\n",
    "def calc_score(rewrite_prompt, rewrite_prompt_pred):\n",
    "    emb = embedding_model.encode([rewrite_prompt, rewrite_prompt_pred], normalize_embeddings=True)\n",
    "    cos_sim = cosine_similarity(emb)[0]\n",
    "    return cos_sim[1]**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewrite_prompt(original_text: str, transformed_text: str):\n",
    "    json_input = json.dumps({\n",
    "        \"original_text\": original_text, \n",
    "        \"rewritten_text\": transformed_text\n",
    "    }, indent=4)\n",
    "    return f\"\"\"<s> [INST]\n",
    "        I will give you a JSON with following structure:\n",
    "        {{\n",
    "            'original_text': 'An original piece of text.'\n",
    "            'rewritten_text': 'A version of original_text that was rewritten by an LLM according to a specific prompt.'\n",
    "        }}\n",
    "\n",
    "        Given the task of understanding how text is rewritten by analyzing the original_text and rewritten_text, your goal is to deduce the specific instructions or prompt that was most likely used to generate the rewritten text from the original text. Consider the changes made in terms of style, tone, structure, and content. Assess whether the rewrite focuses on summarization, paraphrasing, stylistic alteration (e.g., formal to informal), or any specific content changes (e.g., making the text more concise, expanding on ideas, or altering the perspective). Follow this steps:\n",
    "\n",
    "        1. Read the original_text: Start by thoroughly understanding the content, style, tone, and purpose of the original text. Note any key themes, technical terms, and the overall message.\n",
    "        2. Analyze the rewritten_text: Examine how the rewritten text compares to the original. Identify what has been changed, added, or omitted. Pay close attention to changes in style (formal, informal), tone (serious, humorous), structure (paragraph order, sentence structure), and any shifts in perspective or emphasis.\n",
    "        3. Infer the Prompt: Based on your analysis, infer the most likely prompt that guided the rewriting process. Your inference should account for the observed changes in style, tone, structure, and content. Specify the type of task (e.g., summarize, paraphrase, make more accessible to a general audience), any specific directions evident from the changes, and any specific stylistic choice (e.g., 'as a poem', 'as a song', 'in the style of Shakespeare', etc...)\n",
    "\n",
    "        Based on your analysis return the prompt as if you were given the instruction your self like:\n",
    "        \"Rewrite this text...\"\n",
    "        \"Transform this ... into ... based on the style of ...\"\n",
    "        \n",
    "        Make the prompt short and direct using a maximum of 20 words.\n",
    "\n",
    "\n",
    "        Return your answer using the following JSON structure:\n",
    "        {{\"prompt\": \"Your best guess for the prompt used\"}}\n",
    "        \n",
    "\n",
    "            \n",
    "        Return a valid JSON as output and nothing more.\n",
    "        \n",
    "        -----------------------\n",
    "        Input: \n",
    "        \n",
    "        {json_input} [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "def format_response(response):\n",
    "    return f'{{\"prompt\": {response}}} </s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.id.isin(zero_shot_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rewrite_prompt_pred\"] = data.id.progress_apply(lambda x: zero_shot_out[x][\"rewrite_prompt\"])\n",
    "data[\"score\"] = data.progress_apply(lambda x: calc_score(x.rewrite_prompt, x.rewrite_prompt_pred), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Take only the ones where the score < 0.8\n",
    "dpo_data = data.loc[data.score < 0.72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_data[\"split\"] = dpo_data.cluster.apply(lambda x: cluster_to_split.get(str(x), \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_train = []\n",
    "dpo_eval = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in dpo_data.loc[dpo_data.split == \"train\"].iterrows():\n",
    "    chosen_response = format_response(row.rewrite_prompt)\n",
    "    rejected_response = format_response(row.rewrite_prompt_pred)\n",
    "    prompt = get_rewrite_prompt(row.original_text, row.rewritten_text)\n",
    "    dpo_train.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen_response,\n",
    "        \"rejected\": rejected_response,\n",
    "    })\n",
    "\n",
    "for _, row in dpo_data.loc[dpo_data.split == \"val\"].iterrows():\n",
    "    chosen_response = format_response(row.rewrite_prompt)\n",
    "    rejected_response = format_response(row.rewrite_prompt_pred)\n",
    "    prompt = get_rewrite_prompt(row.original_text, row.rewritten_text)\n",
    "    dpo_eval.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen_response,\n",
    "        \"rejected\": rejected_response,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dpo_train, open(\"/home/pbernardo/github/llm-prompt-recovery/data/dpo_train.json\", \"w\"), indent=4)\n",
    "json.dump(dpo_eval, open(\"/home/pbernardo/github/llm-prompt-recovery/data/dpo_eval.json\", \"w\"), indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
