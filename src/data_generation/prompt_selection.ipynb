{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = 0.005\n",
    "DESIRED_DATA_LEN = 1500 # size of the final cluster\n",
    "PROMPTS_SELECTED_JSON = 'prompts_selected.json' # file to save the selected prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"../../data/data.csv\"\n",
    "PROMPT_VARIATONS = \"../../data/prompt_variations.json\"\n",
    "data = pd.read_csv(DATA)\n",
    "prompt_to_cluster = data[[\"rewrite_prompt\", \"cluster\"]].drop_duplicates().set_index(\"rewrite_prompt\")[\"cluster\"].to_dict()\n",
    "prompts_by_cluster = {k: v.tolist() for k, v in data.groupby(\"cluster\")[\"rewrite_prompt\"].unique().to_dict().items()}\n",
    "prompt_variations = json.load(open(PROMPT_VARIATONS))\n",
    "for p, v in prompt_variations.items():\n",
    "    cluster = prompt_to_cluster.get(p, None)\n",
    "    if cluster:\n",
    "        prompts_by_cluster[cluster].extend(v)\n",
    "\n",
    "# Selects a maximum of 20 prompts from each cluster\n",
    "prompts = []\n",
    "for cluster_prompts in prompts_by_cluster.values():\n",
    "    sample_size = min(20, len(cluster_prompts))\n",
    "    prompts.extend(random.sample(cluster_prompts, sample_size))\n",
    "prompts = list(set(prompts))\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = SentenceTransformer('sentence-transformers/sentence-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prompt(prompt):\n",
    "    # Makes lower case and removes punctuation\n",
    "    prompt = prompt.lower()\n",
    "    prompt = ''.join(e for e in prompt if e.isalnum() or e.isspace())\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_distribution = [\n",
    "    {'id': 1, 'rewrite_prompt': 'Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.', 'lb_score': 0.61},\n",
    "    {'id': 2, 'rewrite_prompt': 'Improve the text to this.', 'lb_score': 0.6},    \n",
    "    {'id': 3, 'rewrite_prompt': 'Improve the text.', 'lb_score': 0.59},    \n",
    "    {'id': 4, 'rewrite_prompt': 'Improve that text.', 'lb_score': 0.58},    \n",
    "    {'id': 5, 'rewrite_prompt': 'Rewrite the text into a rhyming, sea-shanty style with a playful tone while maintaining the original information.', 'lb_score': 0.57},    \n",
    "    {'id': 6, 'rewrite_prompt': 'original text elegance clarity improve style word tone meaning', 'lb_score': 0.55},    \n",
    "    {'id': 7, 'rewrite_prompt': 'rephrase new version convey tone', 'lb_score': 0.54},    \n",
    "    {'id': 8, 'rewrite_prompt': 'rephrase increase writing', 'lb_score': 0.53},    \n",
    "    {'id': 9, 'rewrite_prompt': 'Transform the text into a humorous shanty and include a catchy chorus.', 'lb_score': 0.5},\n",
    "    {'id': 10, 'rewrite_prompt': \"Kindly refine the text below to mirror the writing style of , preserving its original intent yet modifying its tone, vocabulary, and stylistic details to resemble the new style. Boost the text's clarity, sophistication, and effectiveness by mimicking the writing style of , keeping the essential meaning unchanged but altering the tone, terminology, and style in accordance with the desired style.\", 'lb_score': 0.58},\n",
    "    {'id': 11, 'rewrite_prompt': \"Please enhance the text provided, emulating the writing style of , while keeping the original intent but changing the tone, vocabulary, and style elements to align with the desired style.\", 'lb_score': 0.59},\n",
    "    {'id': 12, 'rewrite_prompt': \"Kindly refine the subsequent text by adopting the writing style of , preserving its inherent meaning while modifying the tone, language, and stylistic nuances to reflect the new style.\", 'lb_score': 0.56},\n",
    "    {\"id\": 13, 'rewrite_prompt': \"Make this text more negative.\", 'lb_score': 0.45},\n",
    "    {\"id\": 14, 'rewrite_prompt': 'z', \"lb_score\": 0.40},\n",
    "    {\"id\": 15, 'rewrite_prompt': \"The composition stands as a sequence of words arranged for potential contemplation, devoid of explicit intent or discernible purpose. It exists within a framework of neutrality, offering neither direction nor conclusion, inviting observation without expectation. The arrangement facilitates a space for presence, unattached to specific outcomes or interpretations.\", \"lb_score\": 0.44},\n",
    "    {\"id\": 16, 'rewrite_prompt': \"Improve rephrase text manner this written to has character in style.\", \"lb_score\": 0.64},\n",
    "]\n",
    "\n",
    "for entry in tqdm(reference_distribution):\n",
    "    entry['embeddings'] = st_model.encode(entry['rewrite_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_energy(score_on_ref):\n",
    "    # Energy is difference between lb_score and score_on_ref for each entry\n",
    "    return {\n",
    "        entry['id']: abs(entry['lb_score'] - score_on_ref[entry['id']]) for entry in reference_distribution\n",
    "    }\n",
    "\n",
    "def calc_score_on_ref(embed):\n",
    "    return {\n",
    "        entry['id']: cosine_similarity(embed.reshape(1, -1), entry['embeddings'].reshape(1, -1))[0][0]**3 for entry in reference_distribution\n",
    "    }\n",
    "\n",
    "def calc_avg_score_on_ref_from_individual_scores(scores: list[dict]):\n",
    "    res = {}\n",
    "    for ref in reference_distribution:\n",
    "        res[ref['id']] = np.mean([x[ref['id']] for x in scores])\n",
    "    return res\n",
    "\n",
    "def calc_avg_score_on_ref_from_new_score(n_items: int, current_avg_scores: dict, new_score: dict):\n",
    "    res = {}\n",
    "    for ref in reference_distribution:\n",
    "        res[ref['id']] = (current_avg_scores[ref['id']] * n_items + new_score[ref['id']]) / (n_items + 1)\n",
    "    return res\n",
    "\n",
    "def print_report(scores: list[dict]):\n",
    "    avg_scores = calc_avg_score_on_ref_from_individual_scores(scores)\n",
    "    for ref in reference_distribution:\n",
    "        print(f\"Reference {ref['id']}: {ref['lb_score']:.2f} -> {avg_scores[ref['id']]:.2f}\")\n",
    "\n",
    "def calculate_single_energy(n_items, prompt_score, current_avg_scores):\n",
    "    new_energy = calc_energy(calc_avg_score_on_ref_from_new_score(n_items, current_avg_scores, prompt_score))\n",
    "    new_max_energy = np.max(list(new_energy.values()))\n",
    "    return new_max_energy\n",
    "\n",
    "def add_prompt(available_prompts_scores, current_cluster_prompt_scores):\n",
    "    \"\"\"\n",
    "    Greedy selection of the next prompt to add to the cluster\n",
    "    If a prompt is found that makes the cluster energy down return it. Otherwise return None\n",
    "    Conditions:\n",
    "    - If the overall energy is below TH, returns a random prompt that keeps the energy below TH. This is useful to avoid local minima\n",
    "    - If the overall energy is above TH, returns the prompt that makes the energy go down the most\n",
    "    \"\"\"\n",
    "    current_score = calc_avg_score_on_ref_from_individual_scores(current_cluster_prompt_scores)\n",
    "    current_energy = calc_energy(current_score)\n",
    "    best_energy_max = np.max(list(current_energy.values()))\n",
    "    idxs_to_energy_max = []\n",
    "    n_items = len(current_cluster_prompt_scores)\n",
    "    for idx, prompt_score in enumerate(available_prompts_scores):\n",
    "        new_energy = calculate_single_energy(n_items, prompt_score, current_score)\n",
    "        idxs_to_energy_max.append((idx, new_energy))\n",
    "    idxs_to_energy_max.sort(key=lambda x: x[1])\n",
    "    idxs_to_energy_max = [i for i in idxs_to_energy_max if i[1] < best_energy_max or i[1] < TH]\n",
    "    if len(idxs_to_energy_max) == 0:\n",
    "        return None\n",
    "    if best_energy_max <= TH:\n",
    "        #Â If the best energy is already below TH, randomly select one that is below TH\n",
    "        random.choice(idxs_to_energy_max)[0]\n",
    "    return idxs_to_energy_max[0][0]\n",
    "\n",
    "\n",
    "def make_small_cluster(prompts_scores, pbar, desired_len):\n",
    "    \"\"\"\n",
    "    Iterate over the prompts and creates a cluster of prompts that keep the energey below the given\n",
    "    threshold. It will keep adding prompts until the energy is below the threshold, once its below\n",
    "    the threshold it will keep adding prompts that keep the energy below the threshold. If no prompt\n",
    "    is found that makes the energy go down, it will return the current cluster.\n",
    "\n",
    "    This starts from a random prompt, if it fails to find a cluster that keeps the energy below the\n",
    "    threshold, it will restart from a another random prompt and keep trying until it finds an initial\n",
    "    prompt that allows to create a cluster that keeps the energy below the threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    cluster = random.sample(range(len(prompts_scores)), 1)\n",
    "    initial_seeds = [cluster[0]]\n",
    "    available_seeds = [i for i in range(len(prompts_scores)) if i not in initial_seeds]\n",
    "    while True:\n",
    "        cluster_scores = [prompts_scores[i] for i in cluster]\n",
    "        other_prompt_idxs = [i for i in range(len(prompts_scores)) if i not in cluster]\n",
    "        if len(other_prompt_idxs) == 0:\n",
    "            return None\n",
    "        other_prompt_scores = [prompts_scores[i] for i in other_prompt_idxs]\n",
    "        other_prompt_idx = add_prompt(other_prompt_scores, cluster_scores)\n",
    "        if other_prompt_idx is not None:\n",
    "            cluster.append(other_prompt_idxs[other_prompt_idx])\n",
    "            cluster_energy = calc_energy(calc_avg_score_on_ref_from_individual_scores([prompts_scores[i] for i in cluster]))\n",
    "            pbar.set_description(f\"Current cluster size: {str(len(cluster)).zfill(3)} | Energy: {np.max(list(cluster_energy.values())):.4f} | Retries: {len(available_seeds)-1}\")\n",
    "        else:\n",
    "            if np.max(list(cluster_energy.values())) <= TH or len(cluster) >= desired_len:\n",
    "                return cluster\n",
    "            available_seeds = [i for i in range(len(prompts_scores)) if i not in initial_seeds]\n",
    "            if len(available_seeds) == 0:\n",
    "                return None\n",
    "            cluster = random.sample(available_seeds, 1)\n",
    "            initial_seeds.append(cluster[0])\n",
    "            cluster_energy = calc_energy(calc_avg_score_on_ref_from_individual_scores([prompts_scores[i] for i in cluster]))\n",
    "            pbar.set_description(f\"Current cluster size: {str(len(cluster)).zfill(3)} | Energy: {np.max(list(cluster_energy.values())):.4f} | Retries: {len(available_seeds)-1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = st_model.encode(prompts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_scores = [calc_score_on_ref(embed) for embed in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "available_prompts = list(range(len(prompts)))\n",
    "pbar = tqdm(total=DESIRED_DATA_LEN)\n",
    "while len(data) < DESIRED_DATA_LEN:\n",
    "    available_prompts = [i for i in available_prompts if i not in data]\n",
    "    if len(available_prompts) == 0:\n",
    "        break\n",
    "    _prompt_scores = [prompts_scores[i] for i in available_prompts]\n",
    "    missing_len = DESIRED_DATA_LEN - len(data)\n",
    "    _new_cluster_idxs = make_small_cluster(_prompt_scores, pbar, missing_len)\n",
    "    if _new_cluster_idxs is None:\n",
    "        break\n",
    "    _selected_idxs = [available_prompts[i] for i in _new_cluster_idxs]\n",
    "    data.extend(_selected_idxs)\n",
    "    current_selection = [prompts[i] for i in data]\n",
    "    pbar.update(len(_selected_idxs))\n",
    "    json.dump(current_selection, open(PROMPTS_SELECTED_JSON, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
